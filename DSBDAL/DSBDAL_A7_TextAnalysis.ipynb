{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKTi9XxlYGcv2SPeGLwoeS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install textblob"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbsWusNCyqO5","executionInfo":{"status":"ok","timestamp":1709813602188,"user_tz":-330,"elapsed":6622,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"a08340f1-d0aa-42c4-a1fb-be6da6fc520a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n"]}]},{"cell_type":"markdown","source":["Tokenization\n","\n","\n","Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token."],"metadata":{"id":"WWAUkSvq2wuh"}},{"cell_type":"code","source":["import textblob\n","from textblob import TextBlob"],"metadata":{"id":"dReuf8rx0UN0","executionInfo":{"status":"ok","timestamp":1709813603747,"user_tz":-330,"elapsed":1569,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["text = \"Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing.\""],"metadata":{"id":"fKFsHh0s0Yro","executionInfo":{"status":"ok","timestamp":1709813603751,"user_tz":-330,"elapsed":41,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8Rn2U4Loyry","executionInfo":{"status":"ok","timestamp":1709813603751,"user_tz":-330,"elapsed":40,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"38880b2e-6e14-43d5-9847-376e1e9fdd6e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["TextBlob(text).words"],"metadata":{"id":"bkFPvX2G0ixc","executionInfo":{"status":"ok","timestamp":1709813603752,"user_tz":-330,"elapsed":36,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"302499e9-1032-4dde-8bcc-99199eb21268"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["WordList(['Hello', 'everyone', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing'])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import nltk\n","from nltk import sent_tokenize\n","from nltk import word_tokenize"],"metadata":{"id":"rwI_BbWx0lrE","executionInfo":{"status":"ok","timestamp":1709813603752,"user_tz":-330,"elapsed":33,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["tokens_sents = nltk.sent_tokenize(text)\n","print(tokens_sents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVzC5lhX0qS6","executionInfo":{"status":"ok","timestamp":1709813603752,"user_tz":-330,"elapsed":32,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"b31d9155-1eff-4b4f-a2a0-bac39d252522"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello everyone!', 'Welcome to my blog post on Medium.', 'We are studying Natural Language Processing.']\n"]}]},{"cell_type":"code","source":["tokens_words = nltk.word_tokenize(text)\n","print(tokens_words)"],"metadata":{"id":"tXKbM-tT0000","executionInfo":{"status":"ok","timestamp":1709813603752,"user_tz":-330,"elapsed":29,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8c57fd0-f3e6-4219-d74a-296b8ab57752"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'everyone', '!', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n"]}]},{"cell_type":"markdown","source":["Stemming\n","\n","\n","Stemming is definitely the simpler of the two approaches. With stemming, words are reduced to their word stems. A word stem need not be the same root as a dictionary-based morphological root, it just is an equal to or smaller form of the word."],"metadata":{"id":"niM2Uqyx2N34"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer"],"metadata":{"id":"OmOWQQn707nU","executionInfo":{"status":"ok","timestamp":1709813603753,"user_tz":-330,"elapsed":27,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["ps = PorterStemmer()\n","word = (\"civilization\")\n","ps.stem(word)"],"metadata":{"id":"zw7KWh9Z0-Q3","executionInfo":{"status":"ok","timestamp":1709813603753,"user_tz":-330,"elapsed":27,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"b6c37c46-78d9-42cf-e320-9d9f0df8cc4e"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'civil'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from nltk.stem.snowball import SnowballStemmer"],"metadata":{"id":"doAy23eO1Dyn","executionInfo":{"status":"ok","timestamp":1709813603753,"user_tz":-330,"elapsed":25,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["stemmer = SnowballStemmer(language = \"english\")\n","word = \"civilization\"\n","stemmer.stem(word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"UKiBhDFW1GnL","executionInfo":{"status":"ok","timestamp":1709813603753,"user_tz":-330,"elapsed":24,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"c555f501-d099-4ca5-ded3-2c495cf132fc"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'civil'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Lemmatization\n","\n","\n","Lemmatization is the process of finding the form of the related word in the dictionary. It is different from Stemming. It involves longer processes to calculate than Stemming."],"metadata":{"id":"AWmV9gHn2AAu"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()"],"metadata":{"id":"JomfaMbF1JPl","executionInfo":{"status":"ok","timestamp":1709813603754,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rEJV0fotpBQp","executionInfo":{"status":"ok","timestamp":1709813603755,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"a3063981-cb5e-4561-a599-8b0fe974f9e4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# Lemmatize single word\n","\n","print(lemmatizer.lemmatize(\"workers\"))\n","print(lemmatizer.lemmatize(\"beeches\"))"],"metadata":{"id":"7bbj2gSX1R9U","executionInfo":{"status":"ok","timestamp":1709813605777,"user_tz":-330,"elapsed":2042,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9ae0162-2e9a-43d9-f917-25845d7734f8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["worker\n","beech\n"]}]},{"cell_type":"code","source":["text = \"Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize() on each word. \"\n","word_list = nltk.word_tokenize(text)\n","print(word_list)"],"metadata":{"id":"dHenP5Ki1YA5","executionInfo":{"status":"ok","timestamp":1709813605777,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dd8335cf-532e-4ff1-8862-fe87664f7bbe"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['Let', '’', 's', 'lemmatize', 'a', 'simple', 'sentence', '.', 'We', 'first', 'tokenize', 'the', 'sentence', 'into', 'words', 'using', 'nltk.word_tokenize', 'and', 'then', 'we', 'will', 'call', 'lemmatizer.lemmatize', '(', ')', 'on', 'each', 'word', '.']\n"]}]},{"cell_type":"code","source":["lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n","print(lemmatized_output)"],"metadata":{"id":"sEHniJBy1bLt","executionInfo":{"status":"ok","timestamp":1709813605778,"user_tz":-330,"elapsed":21,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb75bb99-5b69-4e68-b7c0-e484223c0d02"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Let ’ s lemmatize a simple sentence . We first tokenize the sentence into word using nltk.word_tokenize and then we will call lemmatizer.lemmatize ( ) on each word .\n"]}]},{"cell_type":"code","source":["# pip install textblob\n","\n","from textblob import TextBlob, Word"],"metadata":{"id":"Psd-F4-Z1eeU","executionInfo":{"status":"ok","timestamp":1709813605778,"user_tz":-330,"elapsed":18,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["word = 'stripes'\n","w = Word(word)\n","w.lemmatize()"],"metadata":{"id":"2XCxzwjB1gjB","executionInfo":{"status":"ok","timestamp":1709813605778,"user_tz":-330,"elapsed":18,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"0f7bad5d-b65b-4744-d079-e8d346a68a92"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'stripe'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["text = \"The striped bats are hanging on their feet for best\"\n","sent = TextBlob(text)\n","\" \". join([w.lemmatize() for w in sent.words])"],"metadata":{"id":"F8rZiI-e1k7n","executionInfo":{"status":"ok","timestamp":1709813605778,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"069f480a-9a45-4971-b0bb-0859f1ca6717"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The striped bat are hanging on their foot for best'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Part Of Speech Tagging (POS Tagging)\n","\n","\n","1 - Part of Speech Tagging (POS-Tag) is the labeling of the words in a text according to their word types (noun, adjective, adverb, verb, etc.)\n","\n","\n","2 - It is a process of converting a sentence to forms — list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on."],"metadata":{"id":"Gf0vkx401mce"}},{"cell_type":"code","source":["import nltk\n","from nltk import word_tokenize"],"metadata":{"id":"y7eIuCvM10rr","executionInfo":{"status":"ok","timestamp":1709813605778,"user_tz":-330,"elapsed":15,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04foq_izpQKt","executionInfo":{"status":"ok","timestamp":1709813605778,"user_tz":-330,"elapsed":14,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"8cb19a59-d5da-4359-940d-1ea341c7b257"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["\"\"\"Parts of Speech (nltk.pos_tag list)\n","CC: It is the conjunction of coordinating\n","CD: It is a digit of cardinal\n","DT: It is the determiner\n","EX: Existential\n","FW: It is a foreign word\n","IN: Preposition and conjunction\n","JJ: Adjective\n","JJR and JJS: Adjective and superlative\n","LS: List marker\n","MD: Modal\n","NN: Singular noun\n","NNS, NNP, NNPS: Proper and plural noun\n","PDT: Predeterminer\n","WRB: Adverb of wh\n","WP$: Possessive wh\n","WP: Pronoun of wh\n","WDT: Determiner of wp\n","VBZ: Verb\n","VBP, VBN, VBG, VBD, VB: Forms of verbs\n","UH: Interjection\n","TO: To go\n","RP: Particle\n","RBS, RB, RBR: Adverb\n","PRP, PRP$: Pronoun personal and professional\n","\"\"\"\n","\n","\n","text = \"The striped bats are hanging on their feet for best\"\n","tokens = nltk.word_tokenize(text)\n","print(\"Parts of Speech: \",nltk.pos_tag(tokens))"],"metadata":{"id":"YQvQ6Vxw13Ef","executionInfo":{"status":"ok","timestamp":1709813607507,"user_tz":-330,"elapsed":1741,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d3cc07e-fd96-49bb-e3af-64e9b842d4a5"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Parts of Speech:  [('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n"]}]},{"cell_type":"markdown","source":["Stop words -\n","\n","The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\n","\n"],"metadata":{"id":"0ZG5tSrqMiOo"}},{"cell_type":"markdown","source":["Libraries to remove stop words -\n","\n","1 - Natural Language Toolkit (NLTK):\n","\n","NLTK is an amazing library to play with natural language."],"metadata":{"id":"8E9kyokvMiZd"}},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKo27nqxpZ1r","executionInfo":{"status":"ok","timestamp":1709813607507,"user_tz":-330,"elapsed":22,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"9b084f2c-e98f-483f-b14e-8f17c9d95102"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","sw_nltk = stopwords.words('english')\n","print(sw_nltk)\n","\n","print()\n","print(len(sw_nltk))"],"metadata":{"id":"bhvIZzAAM_g2","executionInfo":{"status":"ok","timestamp":1709813607507,"user_tz":-330,"elapsed":19,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f8162f2-28e8-4fde-e8ef-0193889f3fc4"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","\n","179\n"]}]},{"cell_type":"code","source":["text = \"When I first met her she was very quiet. She remained quiet during the entire two hour long journey from Stony Brook to New York.\"\n","words = [word for word in text.split() if word.lower() not in sw_nltk]\n","new_text = \" \".join(words)\n","print(new_text)\n","print(\"Old length: \", len(text))\n","print(\"New length: \", len(new_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bavqT2BdsUys","executionInfo":{"status":"ok","timestamp":1709813607508,"user_tz":-330,"elapsed":15,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"f84d55cb-8383-4298-ef5b-fb9053a42a7c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["first met quiet. remained quiet entire two hour long journey Stony Brook New York.\n","Old length:  129\n","New length:  82\n"]}]},{"cell_type":"markdown","source":["2 - spaCy:\n","\n","spaCy is an open-source software library for advanced NLP. This library is quite popular now and NLP practitioners use this to get their work done in the best way."],"metadata":{"id":"kvrLsrgwswJm"}},{"cell_type":"code","source":["import spacy\n","#loading the english language small model of spacy\n","en = spacy.load('en_core_web_sm')\n","sw_spacy = en.Defaults.stop_words\n","print(sw_spacy)\n","\n","print()\n","print(len(sw_spacy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NULL0D1Wsej6","executionInfo":{"status":"ok","timestamp":1709813612717,"user_tz":-330,"elapsed":5218,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"d9eec19f-94cf-457d-b368-b1e2f52a9ba6"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["{'whenever', '’ve', '‘s', 'anyway', 'within', 'again', 'his', 'n‘t', 'not', 'used', 'cannot', 'almost', 'hereupon', 'amount', 'meanwhile', 'using', 'been', 'among', 'ourselves', 'ever', '‘d', 'for', 'always', 'on', 'may', 'back', 'off', 'should', 'if', 'elsewhere', 'myself', 'down', 'hence', 'many', 'all', 'moreover', 'however', 'herein', 'as', 'they', 'himself', 'whence', 'i', 'thence', 'both', 'can', 'became', 'an', 'did', 'say', 'has', 'hereby', 'does', 'thereupon', 'least', 'three', 'would', 'us', 'amongst', 'besides', 'whether', '’m', 'these', 'where', 'fifty', 'her', 'made', 'herself', 'next', 'wherever', 'you', 'further', 'hundred', 'might', 'same', 'between', 'other', 'part', 'mostly', 'top', 'towards', 'eight', 'out', 'to', 'someone', 'do', 'is', '’d', 'but', 'bottom', 'former', 'are', 'why', 'give', 'alone', 'against', 'anyone', 'the', 'whither', 'somehow', 'own', 'perhaps', 'thru', 'into', 'above', 'few', 'together', 'will', 'sometime', 'hers', 'else', '‘re', 'those', 'either', 'whoever', 'due', 'he', 'whereas', 'yourselves', 'just', 'well', 'more', 'because', 'formerly', 'themselves', 'whereupon', 'put', 'were', 'forty', 'after', 'when', '‘ll', 'although', 'seeming', 'full', '‘m', 'or', 'mine', 'upon', 'done', 'wherein', 'others', 'which', 'such', 'neither', 'nothing', 'thereby', 'was', '’re', 'below', 'whom', 'become', 'have', 'here', 'about', 'now', 'much', 'becoming', 'everyone', 'doing', 'through', '‘ve', 'really', 'noone', \"'s\", 'their', 'side', 'afterwards', 'yet', 'during', 'everything', 'please', 'nor', 'since', 'make', 'anywhere', 'somewhere', 'along', 'therein', 'per', 'latter', 'without', 'see', 'only', 'at', 'them', 'beforehand', 'it', 'then', \"'ve\", 'never', 'she', 'hereafter', 'several', 'so', 'with', 'until', 'whereafter', 'something', 'around', 'before', 'none', 'am', 'get', 'our', 're', 'than', \"'ll\", 'some', 'ours', 'across', 'a', 'regarding', 'seemed', 'n’t', 'nevertheless', 'nobody', 'whole', 'twelve', 'most', 'therefore', 'nine', 'twenty', 'empty', 'less', 'various', 'sometimes', 'ca', 'over', 'being', 'go', 'and', 'had', 'third', 'rather', 'we', 'beside', 'seems', 'four', 'two', 'namely', 'must', 'sixty', 'under', 'latterly', 'show', 'beyond', 'any', 'six', 'everywhere', 'becomes', 'one', 'toward', 'already', 'thereafter', 'him', 'could', 'seem', 'though', \"'d\", 'there', 'serious', 'what', 'enough', 'from', 'whatever', 'another', 'first', 'behind', 'otherwise', 'unless', \"'m\", 'ten', 'nowhere', 'anything', 'front', 'fifteen', 'by', 'except', 'every', 'very', '’s', 'eleven', 'call', 'too', \"n't\", 'keep', \"'re\", 'last', 'while', 'up', 'this', 'quite', 'also', '’ll', 'how', 'even', 'whereby', 'your', 'who', 'name', 'still', 'five', 'that', 'via', 'indeed', 'my', 'its', 'often', 'thus', 'yours', 'whose', 'no', 'itself', 'yourself', 'move', 'throughout', 'onto', 'anyhow', 'take', 'me', 'of', 'in', 'once', 'be', 'each'}\n","\n","326\n"]}]},{"cell_type":"code","source":["words = [word for word in text.split() if word.lower() not in sw_spacy]\n","new_text = \" \".join(words)\n","print(new_text)\n","print(\"Old length: \", len(text))\n","print(\"New length: \", len(new_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9e1zuo5tPEU","executionInfo":{"status":"ok","timestamp":1709813613866,"user_tz":-330,"elapsed":1178,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"d4dd9682-3a5c-41f1-e6f2-a289b292cbd5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["met quiet. remained quiet entire hour long journey Stony Brook New York.\n","Old length:  129\n","New length:  72\n"]}]},{"cell_type":"markdown","source":["3 - Gensim:\n","\n","Gensim (Generate Similar) is an open-source software library that uses modern statistical machine learning. According to Wikipedia, Gensim is designed to handle large text collections using data streaming and incremental online algorithms, which differentiates it from most other machine learning software packages that target only in-memory processing."],"metadata":{"id":"RfDD04hxtcc-"}},{"cell_type":"code","source":["import gensim\n","from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n","print(STOPWORDS)\n","\n","print()\n","print(len(STOPWORDS))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2oSDWTvtqnM","executionInfo":{"status":"ok","timestamp":1709813613867,"user_tz":-330,"elapsed":36,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"87e308d0-5e79-47e3-edaa-74486b8ed8f9"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["frozenset({'whenever', 'anyway', 'within', 'his', 'again', 'not', 'used', 'cannot', 'almost', 'hereupon', 'amount', 'using', 'meanwhile', 'been', 'among', 'ourselves', 'ever', 'for', 'always', 'on', 'may', 'back', 'thin', 'off', 'should', 'if', 'elsewhere', 'myself', 'down', 'hence', 'many', 'all', 'moreover', 'however', 'bill', 'herein', 'they', 'as', 'himself', 'whence', 'i', 'thence', 'both', 'can', 'interest', 'did', 'became', 'an', 'say', 'has', 'hereby', 'does', 'thereupon', 'least', 'three', 'would', 'sincere', 'found', 'fire', 'us', 'amongst', 'besides', 'whether', 'these', 'where', 'fifty', 'her', 'made', 'next', 'herself', 'wherever', 'you', 'further', 'hundred', 'thick', 'might', 'same', 'between', 'other', 'part', 'mostly', 'top', 'towards', 'eight', 'out', 'to', 'someone', 'do', 'is', 'system', 'but', 'bottom', 'former', 'are', 'why', 'give', 'alone', 'against', 'anyone', 'the', 'whither', 'somehow', 'don', 'own', 'perhaps', 'thru', 'into', 'above', 'few', 'ltd', 'describe', 'together', 'will', 'sometime', 'hers', 'else', 'those', 'either', 'whoever', 'due', 'he', 'whereas', 'yourselves', 'just', 'well', 'more', 'because', 'formerly', 'themselves', 'whereupon', 'put', 'were', 'forty', 'after', 'when', 'although', 'computer', 'seeming', 'full', 'or', 'mine', 'find', 'upon', 'done', 'wherein', 'others', 'which', 'such', 'neither', 'kg', 'nothing', 'thereby', 'was', 'below', 'whom', 'become', 'have', 'here', 'about', 'now', 'much', 'becoming', 'everyone', 'doing', 'through', 'couldnt', 'really', 'noone', 'their', 'side', 'afterwards', 'yet', 'during', 'doesn', 'fill', 'everything', 'please', 'somewhere', 'nor', 'since', 'anywhere', 'make', 'along', 'therein', 'per', 'latter', 'without', 'see', 'only', 'at', 'them', 'beforehand', 'it', 'then', 'never', 'she', 'hereafter', 'several', 'so', 'ie', 'with', 'until', 'whereafter', 'con', 'something', 'before', 'around', 'none', 'am', 'get', 'our', 're', 'than', 'some', 'ours', 'across', 'regarding', 'a', 'seemed', 'nevertheless', 'nobody', 'whole', 'didn', 'twelve', 'therefore', 'most', 'twenty', 'nine', 'empty', 'less', 'various', 'sometimes', 'over', 'being', 'go', 'and', 'had', 'third', 'rather', 'we', 'beside', 'seems', 'four', 'two', 'namely', 'must', 'sixty', 'under', 'latterly', 'un', 'show', 'beyond', 'any', 'six', 'everywhere', 'toward', 'one', 'becomes', 'already', 'thereafter', 'him', 'could', 'seem', 'though', 'km', 'there', 'serious', 'inc', 'what', 'enough', 'from', 'whatever', 'another', 'first', 'behind', 'otherwise', 'unless', 'etc', 'ten', 'nowhere', 'eg', 'anything', 'front', 'amoungst', 'fifteen', 'by', 'except', 'hasnt', 'very', 'every', 'eleven', 'call', 'too', 'cry', 'keep', 'last', 'while', 'mill', 'up', 'this', 'quite', 'also', 'how', 'even', 'whereby', 'your', 'who', 'name', 'still', 'five', 'that', 'via', 'co', 'indeed', 'de', 'my', 'its', 'thus', 'often', 'yours', 'whose', 'no', 'itself', 'yourself', 'move', 'throughout', 'onto', 'anyhow', 'cant', 'take', 'detail', 'me', 'of', 'in', 'once', 'be', 'each'})\n","\n","337\n"]}]},{"cell_type":"code","source":["new_text = remove_stopwords(text)\n","print(new_text)\n","print(\"Old length: \", len(text))\n","print(\"New length: \", len(new_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZ-h1BOYt72L","executionInfo":{"status":"ok","timestamp":1709813613867,"user_tz":-330,"elapsed":31,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"46b26e5e-6b5c-45f1-a29b-e9ba8391f175"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["When I met quiet. She remained quiet entire hour long journey Stony Brook New York.\n","Old length:  129\n","New length:  83\n"]}]},{"cell_type":"markdown","source":["4 - Scikit-Learn:\n","\n","Scikit-Learn needs no introduction. It is a free software machine learning library for Python. It is probably the most powerful library for machine learning."],"metadata":{"id":"ksZ_qJrcuDpm"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n","print(ENGLISH_STOP_WORDS)\n","\n","print()\n","print(len(ENGLISH_STOP_WORDS))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ji0LORQfuFOO","executionInfo":{"status":"ok","timestamp":1709813613867,"user_tz":-330,"elapsed":28,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"9674a562-0e3e-450a-94d8-702248af0bf3"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["frozenset({'whenever', 'anyway', 'within', 'again', 'his', 'not', 'cannot', 'almost', 'hereupon', 'amount', 'meanwhile', 'been', 'among', 'ourselves', 'ever', 'for', 'always', 'on', 'may', 'back', 'thin', 'off', 'should', 'if', 'elsewhere', 'myself', 'down', 'hence', 'many', 'all', 'moreover', 'however', 'bill', 'herein', 'as', 'they', 'himself', 'whence', 'i', 'thence', 'both', 'can', 'interest', 'became', 'an', 'has', 'hereby', 'thereupon', 'least', 'three', 'would', 'sincere', 'found', 'fire', 'us', 'amongst', 'besides', 'whether', 'these', 'where', 'fifty', 'her', 'made', 'herself', 'next', 'wherever', 'you', 'further', 'hundred', 'thick', 'might', 'same', 'between', 'other', 'part', 'mostly', 'top', 'towards', 'eight', 'out', 'to', 'someone', 'do', 'is', 'system', 'but', 'bottom', 'former', 'are', 'why', 'give', 'alone', 'against', 'anyone', 'the', 'whither', 'somehow', 'own', 'perhaps', 'thru', 'into', 'above', 'few', 'ltd', 'describe', 'together', 'will', 'sometime', 'hers', 'else', 'those', 'either', 'whoever', 'due', 'he', 'whereas', 'yourselves', 'well', 'more', 'because', 'formerly', 'themselves', 'whereupon', 'put', 'were', 'forty', 'after', 'when', 'although', 'seeming', 'full', 'or', 'mine', 'find', 'upon', 'done', 'wherein', 'others', 'which', 'such', 'neither', 'nothing', 'thereby', 'was', 'below', 'whom', 'become', 'have', 'here', 'about', 'now', 'much', 'becoming', 'everyone', 'through', 'couldnt', 'noone', 'their', 'side', 'afterwards', 'yet', 'during', 'fill', 'everything', 'please', 'nor', 'since', 'somewhere', 'anywhere', 'along', 'therein', 'per', 'latter', 'without', 'see', 'only', 'at', 'them', 'beforehand', 'it', 'then', 'never', 'she', 'hereafter', 'several', 'so', 'ie', 'with', 'until', 'whereafter', 'con', 'something', 'around', 'before', 'none', 'am', 'get', 'our', 're', 'than', 'some', 'ours', 'across', 'a', 'seemed', 'nevertheless', 'nobody', 'whole', 'twelve', 'most', 'therefore', 'nine', 'twenty', 'empty', 'less', 'sometimes', 'over', 'being', 'go', 'and', 'had', 'third', 'rather', 'we', 'beside', 'seems', 'four', 'two', 'namely', 'must', 'sixty', 'under', 'latterly', 'un', 'show', 'beyond', 'any', 'six', 'everywhere', 'becomes', 'one', 'toward', 'already', 'thereafter', 'him', 'could', 'seem', 'though', 'there', 'serious', 'inc', 'what', 'enough', 'from', 'whatever', 'another', 'first', 'behind', 'otherwise', 'etc', 'ten', 'nowhere', 'eg', 'anything', 'front', 'amoungst', 'fifteen', 'by', 'except', 'hasnt', 'every', 'very', 'eleven', 'call', 'too', 'cry', 'keep', 'last', 'while', 'mill', 'up', 'this', 'also', 'how', 'even', 'whereby', 'your', 'who', 'name', 'still', 'five', 'that', 'via', 'co', 'indeed', 'de', 'my', 'its', 'often', 'thus', 'yours', 'whose', 'no', 'itself', 'yourself', 'move', 'throughout', 'onto', 'anyhow', 'cant', 'take', 'detail', 'me', 'of', 'in', 'once', 'be', 'each'})\n","\n","318\n"]}]},{"cell_type":"code","source":["words = [word for word in text.split() if word.lower() not in ENGLISH_STOP_WORDS]\n","new_text = \" \".join(words)\n","print(new_text)\n","print(\"Old length: \", len(text))\n","print(\"New length: \", len(new_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wc7PUfoBuLA7","executionInfo":{"status":"ok","timestamp":1709813613867,"user_tz":-330,"elapsed":24,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"a05a1a2e-db98-4d8c-a5e2-9c98522f432c"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["met quiet. remained quiet entire hour long journey Stony Brook New York.\n","Old length:  129\n","New length:  72\n"]}]},{"cell_type":"code","source":["sw_nltk.extend(['first', 'second', 'third', 'me'])\n","print(len(sw_nltk))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZAt5tWhudxC","executionInfo":{"status":"ok","timestamp":1709813613868,"user_tz":-330,"elapsed":20,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"81dacddf-12ea-4d26-9071-0172d7673209"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["183\n"]}]},{"cell_type":"code","source":["sw_nltk.remove('not')\n","print(len(sw_nltk))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqpigsXSuixS","executionInfo":{"status":"ok","timestamp":1709813613868,"user_tz":-330,"elapsed":17,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"fde78eef-0915-4864-fb5b-fa055eddbcf5"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["182\n"]}]},{"cell_type":"markdown","source":["Custom Stop Words Removal -\n","\n","\n","If we do not want to use any of these libraries, we can also create our own custom stop words list and use it in our task. This is usually done when we have domain expertise in our field and when we know which words we should avoid while performing our task."],"metadata":{"id":"h1VQAQ6Nu8Gw"}},{"cell_type":"code","source":["my_stop_words = ['her','me','i','she','it']\n","words = [word for word in text.split() if word.lower() not in my_stop_words]\n","new_text = \" \".join(words)\n","print(new_text)\n","print(\"Old length: \", len(text))\n","print(\"New length: \", len(new_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deThGiX1u0L0","executionInfo":{"status":"ok","timestamp":1709813613868,"user_tz":-330,"elapsed":15,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"1ec78477-3674-4b28-eaa0-31eafcf11804"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["When first met was very quiet. remained quiet during the entire two hour long journey from Stony Brook to New York.\n","Old length:  129\n","New length:  115\n"]}]},{"cell_type":"markdown","source":["TF-IDF Vectorizer"],"metadata":{"id":"NBccEdWk3tdl"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","train = ('The sky is blue.','The sun is bright.')\n","test = ('The sun in the sky is bright', 'We can see the shining sun, the bright sun.')\n","# instantiate the vectorizer object\n","# use analyzer is word and stop_words is english which are responsible for remove stop words and create word vocabulary\n","tfidfvectorizer = TfidfVectorizer(analyzer='word' , stop_words='english',)\n","tfidfvectorizer.fit(train)\n","tfidf_train = tfidfvectorizer.transform(train)\n","tfidf_term_vectors  = tfidfvectorizer.transform(test)\n","print(\"Sparse Matrix form of test data : \\n\")\n","tfidf_term_vectors.todense()"],"metadata":{"id":"3XKi8Iq94Mca","executionInfo":{"status":"ok","timestamp":1709814370017,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vivek Gotecha","userId":"00337109865623423029"}},"outputId":"244ad3b6-ece9-4c69-e870-f68a67c87fb3","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Sparse Matrix form of test data : \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["matrix([[0.        , 0.57735027, 0.57735027, 0.57735027],\n","        [0.        , 0.4472136 , 0.        , 0.89442719]])"]},"metadata":{},"execution_count":39}]}]}